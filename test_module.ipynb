{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15875f39e88e42ba84b1250d5a9dae01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff207b9e0a0c4fc8b4acaeb4425fefe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "jnlpba.py:   0%|          | 0.00/4.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4a28b50a484ba9813b28e3ab450944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b998c0c78d9043ee87540734c3e85b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/863k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2302705dde08420eabcfad5da7b0a800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/18546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b4783e34f74f78b03d234ffbeaab45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3856 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 18546\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3856\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "load_dataset(\"jnlpba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhu.3723/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/zhu.3723/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/zhu.3723/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "Lora(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): LoraLinear(\n",
      "      (base_layer): LoraLinear(\n",
      "        (base_layer): Linear(in_features=512, out_features=1000, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import lora\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "print(resnet18)\n",
    "lora_resnet18 = lora.Lora(resnet18)\n",
    "lora_resnet18._apply_lora(lora_resnet18.model.fc)\n",
    "print(lora_resnet18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pseudo_label tensor([[-0.5910, -0.3493, -0.6492, -1.5522, -0.7644, -0.2025, -0.5300,  0.5564,\n",
      "          0.4337, -0.6773, -1.0394, -0.9122, -0.3085, -0.9754, -1.1184, -0.5930,\n",
      "         -0.9025, -0.2865, -0.5246, -0.5870, -1.5653, -0.7951, -1.3831,  0.1732,\n",
      "         -0.9422, -1.0763, -0.7888, -1.1063, -0.8880, -0.3786, -0.9136, -0.8250,\n",
      "         -0.5466, -0.5231, -0.3847, -0.4555,  0.5807, -0.7896, -0.5481, -0.0846,\n",
      "         -0.7958, -0.9499, -1.0525, -0.4302, -0.7772, -0.4823, -0.8727, -0.7115,\n",
      "         -1.2726, -1.0723, -0.4903,  0.4098, -0.4022, -0.6452, -0.3013, -1.2424,\n",
      "         -0.3215, -1.4357, -0.5682, -0.5034,  0.7215,  0.1160, -0.1543,  0.1352,\n",
      "         -0.8648, -0.2945, -0.3526, -0.3892, -0.8371, -1.0664, -1.5108,  0.1204,\n",
      "         -1.4080, -0.2616, -1.1177, -1.2667,  0.1820, -0.4926,  0.2412,  0.1508,\n",
      "         -0.7806, -1.5508, -0.1182, -0.7657, -0.3906, -0.1404,  0.0203,  0.2669,\n",
      "         -0.0567, -0.7683, -1.1666, -1.0871, -1.8775, -0.2152,  0.1756, -2.0806,\n",
      "         -0.6686, -0.4708, -1.4849, -0.2967, -1.1142, -1.0818, -0.8973, -0.2560,\n",
      "         -0.1391, -0.6139, -0.4664, -1.2947, -0.9712, -1.4235, -1.0970, -0.5262,\n",
      "          1.0517,  0.3636,  0.4469, -0.9238, -0.6826, -0.2062,  0.5989, -0.3156,\n",
      "         -0.7723,  0.0285,  0.3495,  0.0885,  1.0412, -0.0815,  0.3289, -1.4425,\n",
      "         -1.1920, -1.1289, -1.3214, -1.5182, -1.0476, -1.4550, -0.6679, -1.2668,\n",
      "         -0.9236, -1.2585, -1.3719, -1.5332, -1.5576, -1.7052, -2.1220, -1.6221,\n",
      "         -0.6581, -0.4069, -1.0184, -1.8567, -1.2210, -1.3513,  0.3580,  1.4857,\n",
      "         -1.0160, -0.4444,  0.0177,  0.1842, -0.3480, -0.2375,  0.1045,  0.1359,\n",
      "          0.4361,  0.6774,  0.3253,  0.5769,  0.4108, -0.1645, -0.1727, -0.3692,\n",
      "          0.5646, -0.1608, -0.0505,  0.8042,  0.4362,  0.2705,  0.2127, -0.6736,\n",
      "          0.0902,  0.0324,  0.5247,  0.5339,  0.4628,  0.0154,  0.5882,  0.1792,\n",
      "          0.6155,  0.7489,  0.6318,  0.2246,  0.1879,  0.6481, -0.5192,  0.4784,\n",
      "          0.3377,  0.5593, -0.6206,  0.7710,  0.1611,  0.2151,  0.2437,  0.7010,\n",
      "          0.0730,  0.2119,  0.6086,  0.5876,  0.1016,  0.2343,  0.0242,  0.5538,\n",
      "          1.2148,  0.4499, -0.0830,  0.4102,  0.3174, -0.0734,  0.0288,  0.2362,\n",
      "         -0.1315,  0.2791, -0.3846,  0.6112,  0.1077, -0.0788,  0.0468,  0.7144,\n",
      "          0.3131,  0.3902,  0.0411,  0.6993, -0.3673, -0.1314, -0.0185,  0.3962,\n",
      "          0.3668, -0.0263,  0.7962,  0.8071,  0.4797,  0.4242,  0.6827, -0.0566,\n",
      "          0.5330, -0.0839,  0.4146,  0.2732, -0.3051,  0.3865,  0.4241,  0.0488,\n",
      "          0.6661,  0.2477,  0.4519,  0.6394, -0.8182,  0.6283,  0.8889, -0.6130,\n",
      "          0.4249,  0.2215, -0.1491,  0.2239, -0.2315, -0.5889, -0.3300,  0.3574,\n",
      "          0.7588,  0.7443,  0.3443,  0.6145, -0.1303, -0.3536, -0.8159, -0.9704,\n",
      "         -0.4956,  0.6442, -1.0820, -0.9617, -1.0209, -0.7493, -1.1437, -0.6102,\n",
      "         -0.3035,  0.8869,  0.8525,  0.0426,  0.3738,  0.8780, -0.2041, -0.2793,\n",
      "         -0.6940, -1.5471, -0.9559, -1.1758, -0.3111, -0.9385, -0.9113, -0.8795,\n",
      "         -0.9046, -1.4215, -0.4005, -0.1470, -1.9335, -0.6864, -0.5096, -0.3212,\n",
      "         -1.1625, -0.9190,  0.1194, -0.7528, -1.3510, -0.4275,  0.4368, -0.4638,\n",
      "         -0.5441,  0.2590,  0.6984, -0.5461, -0.8736, -1.1988, -1.1319, -0.7507,\n",
      "         -1.5510, -1.1622, -1.3948, -1.5779, -1.4228, -1.6157, -1.3135,  0.1024,\n",
      "         -0.1037, -0.4495,  0.0210, -0.1498,  0.0183,  0.2551, -0.5084, -0.8537,\n",
      "         -1.5081,  0.1391,  0.7526, -1.1888, -0.5619,  0.6215, -0.3913, -1.4333,\n",
      "         -0.7639,  0.7756, -0.6545, -1.4387, -0.0548, -1.2948, -1.1411, -2.2843,\n",
      "         -1.3798, -0.8341, -0.5922,  0.5187,  1.0165, -0.0429,  0.4024,  0.3717,\n",
      "         -0.1319,  0.5082,  0.1079,  0.2029, -0.4963, -0.5487, -1.0324, -0.3473,\n",
      "         -0.8179, -0.6434, -0.6291, -0.2337, -0.4432,  0.0040, -0.3070, -0.9832,\n",
      "         -1.1196,  0.2096, -0.3274, -0.5144,  0.2964, -0.4638, -0.2809, -0.6613,\n",
      "         -0.8375, -0.5103, -1.2132, -1.0329, -0.9145, -0.1910,  0.6003,  0.1978,\n",
      "         -1.5058, -1.6584, -0.3340,  0.5019, -1.1483, -0.6891,  0.4328,  0.0652,\n",
      "         -0.7861,  0.6869,  0.2212, -2.1379, -1.8957, -0.7266, -0.2434, -0.2160,\n",
      "         -0.2070,  0.9685, -0.1432,  0.2671,  2.0799,  0.6874,  0.3194,  0.8215,\n",
      "         -0.2945,  0.2909,  0.2311,  1.0412,  0.7599,  1.1914, -0.2394,  0.1375,\n",
      "          0.1152, -0.9836, -0.0197,  1.5343,  1.7636,  0.5031, -0.6483, -0.0464,\n",
      "          0.2831,  0.5399,  0.5421,  1.0228, -0.3368, -0.4112,  0.3696,  0.2921,\n",
      "          0.9007,  0.4926,  0.0166, -0.4674, -0.3375,  0.1701,  0.3653,  1.4897,\n",
      "          0.9694, -0.6194, -0.2586,  0.3988,  0.5733, -0.2053, -0.2520,  0.7013,\n",
      "          1.4808,  1.3448, -0.1112,  0.7687, -0.8231,  0.5347,  1.4049,  2.4975,\n",
      "          0.9462, -0.3303, -1.3948, -0.0319,  0.0237,  1.6052,  1.1633,  0.5463,\n",
      "          0.2525,  0.8325, -0.1912, -0.0621,  0.1325,  0.4435,  0.8035,  0.4594,\n",
      "         -0.1387,  0.1255,  0.2454, -0.8645, -1.5252, -0.0561, -0.2543,  1.1823,\n",
      "          1.5993,  1.1919,  0.4666,  0.7685,  0.7354, -1.1102,  1.2642, -0.9991,\n",
      "          0.0451, -0.3911, -0.3400,  1.2548, -1.5762,  0.5411,  1.3881,  0.4532,\n",
      "          1.0721,  1.1350,  1.0173,  0.5990,  0.4837,  0.3462, -1.1001, -0.9395,\n",
      "          0.6611,  0.1836,  1.0801,  1.7866,  0.3781,  0.1050,  1.2720,  0.7376,\n",
      "         -0.7932,  0.3944,  0.9121,  1.6567,  0.2867, -0.7801, -0.1347, -0.5028,\n",
      "          0.2617,  0.0729,  0.9400,  0.1613,  0.0145, -0.9251,  0.4255, -0.6046,\n",
      "         -0.5330, -0.6041,  0.1023,  1.2854, -1.2638,  1.6727,  1.1731,  0.7904,\n",
      "          0.5938,  0.8008,  0.6202, -2.1687, -1.3326, -0.0423, -0.5298,  0.1692,\n",
      "          0.7784, -0.1097, -1.5910, -0.6683,  0.1845,  0.3109,  1.2262,  0.8735,\n",
      "         -0.0781, -0.2146,  0.9655,  0.0667, -1.2067, -0.8715,  0.1192,  1.0810,\n",
      "          0.5305, -0.4646,  0.9856, -0.0274,  0.8960, -0.7382,  0.4955, -0.1228,\n",
      "         -0.9609,  1.1827,  0.3288,  0.2407,  0.1179, -0.2218,  0.8585,  0.5046,\n",
      "          0.8446,  0.8382, -0.4519,  1.6509,  1.1587,  1.1876, -0.6273,  0.6378,\n",
      "         -0.5593,  0.7417,  0.2732, -0.6843,  0.9836, -0.3322, -0.6236,  0.9455,\n",
      "          2.1747, -0.1443, -0.0647, -0.4716,  0.5488,  0.1405,  1.2349, -0.4449,\n",
      "          0.6110, -0.2727,  0.9287,  0.7153, -0.5738,  0.6392,  0.1331,  0.3189,\n",
      "          1.1441,  0.5625,  2.0388,  0.8692,  1.0319,  0.7121,  0.2640,  0.4975,\n",
      "          0.0817, -1.1239,  1.0573, -0.4225, -1.1803,  0.3168, -0.0083,  0.9627,\n",
      "          0.7297,  1.1258, -0.3075,  0.1976,  1.2292,  0.8550,  0.6862,  0.2012,\n",
      "         -1.6609,  1.2508,  0.0783,  1.3981,  0.8378, -0.7819,  0.6057,  0.4531,\n",
      "         -0.6386, -1.5051,  1.1282,  0.0745,  0.6617,  0.8796, -0.0801,  0.6076,\n",
      "          0.0074,  0.0460,  0.1369,  0.4350, -0.0652, -1.0876, -0.1112, -0.8758,\n",
      "          0.8702,  0.0667,  1.2292,  0.4211, -0.7963, -0.4513,  0.3214, -0.2575,\n",
      "         -0.2250,  0.6519,  1.3034, -0.7725,  1.5268,  1.0729,  0.9147,  0.2689,\n",
      "          0.6775,  0.5711, -0.5430,  0.4930,  0.9248, -1.4715, -0.0529, -1.0835,\n",
      "         -0.3133, -0.8583, -0.5401,  0.7228,  0.9848,  0.7876, -1.0300,  0.9298,\n",
      "          1.6981,  0.1446, -0.4327,  0.6642,  1.8400, -0.3169, -0.2683,  0.4119,\n",
      "          0.6976, -0.4730, -0.3717,  0.2159,  0.8148,  0.2064,  0.8942,  1.0004,\n",
      "          0.1089, -0.5156,  0.4204, -0.4346,  0.7271, -0.7291, -0.3757,  0.6769,\n",
      "          0.3250,  0.2375,  1.4812,  0.2405, -0.7588,  1.3691, -0.7202, -0.1875,\n",
      "          1.4351, -0.4923,  0.2046,  2.2815, -0.8323,  1.7892, -1.4466,  0.1343,\n",
      "         -0.0694,  0.7177,  0.9727,  0.1415,  1.1493, -0.0472,  0.3083,  0.2095,\n",
      "          0.7020,  0.0930,  0.2107,  0.7862,  0.7227,  1.4905,  0.5043, -0.0684,\n",
      "          0.2280,  0.6407,  0.8052, -0.6088,  1.0920, -0.0704,  1.4612, -0.1339,\n",
      "          0.1230,  0.8021,  0.4600,  0.6397,  1.2206,  0.7402,  0.3949,  0.5460,\n",
      "         -0.1537,  1.2840,  0.4610,  0.3475,  1.5331,  0.8505,  0.8864,  0.4370,\n",
      "          0.4211,  0.5486,  1.4386, -0.7066, -1.0890, -0.8519,  1.0041,  0.8194,\n",
      "          1.6289,  0.2959,  0.4390,  1.2518,  0.3665, -0.0670,  0.7536,  1.0888,\n",
      "          1.5893,  0.9244,  0.3475,  0.2347,  0.8342,  0.8522, -0.8576,  0.4108,\n",
      "         -0.8679,  0.1381, -1.1252, -1.2257,  1.0281,  1.2405,  0.3533,  0.2261,\n",
      "          1.3834,  0.2616, -0.5340,  1.0937, -0.3645,  1.6963, -1.0615, -0.2055,\n",
      "          0.3748, -1.0965,  1.6636,  0.4418, -1.7691, -1.0582,  0.4353,  0.7016,\n",
      "          0.9817, -0.8226,  0.3083,  0.8153,  1.3374, -0.6869,  1.1687,  0.3635,\n",
      "         -0.7212, -1.1079,  0.1144,  0.6220,  1.5958,  1.6138,  1.1066, -0.6684,\n",
      "          1.4431,  0.4814,  0.5243,  0.6527,  0.6506,  1.6858,  0.7032, -0.5495,\n",
      "          0.3508,  0.9987,  1.3451,  1.3241,  1.6896, -0.5321, -0.2662,  0.8622,\n",
      "         -0.6497, -0.2744, -0.2057,  0.9126,  0.1722,  1.3857,  1.0155, -0.1665,\n",
      "         -0.3893,  0.6024,  0.1280, -0.3919,  1.5049, -0.4224,  0.9586, -1.4685,\n",
      "          1.0740, -1.1125, -2.3457,  0.1799,  1.5624, -0.3114, -0.2208,  1.6531,\n",
      "          0.9539, -0.2780,  1.0600,  1.4034,  0.0859,  0.2605, -0.3945, -0.2851,\n",
      "         -1.0537,  0.3447, -0.4125,  0.1751,  0.7869,  0.0925, -0.5730, -0.7163,\n",
      "          1.2140,  0.5969,  1.9823,  2.0136, -1.1087, -0.4367,  1.5350,  0.8639,\n",
      "          0.8922,  0.0109, -0.3952,  1.3664, -0.9823,  0.9956,  1.3890,  1.1784,\n",
      "          0.7086, -0.4784, -1.8339, -0.3863,  0.1607,  0.3470,  0.4791,  0.1799,\n",
      "         -0.2380,  1.1612, -0.6803,  0.6059, -0.2753, -0.8989, -0.9728, -0.5405,\n",
      "         -0.1921,  1.4842, -0.2295,  0.1270,  0.4841, -1.6089,  0.0119, -0.4518,\n",
      "          0.3837,  0.2483, -0.0062,  0.0412, -0.2253, -0.5273, -0.1795,  0.2731,\n",
      "         -0.4714, -0.7034, -1.1401,  0.6095,  0.7750, -0.2905,  0.2120, -0.4486,\n",
      "         -0.4382,  0.2971,  0.8743, -0.3884, -0.1913, -0.3573,  0.0094, -0.9315,\n",
      "          0.3144,  0.3430, -0.3514, -0.7436, -1.1696, -0.1809,  0.5368, -0.4209,\n",
      "          0.9203,  0.0958, -0.1064,  1.1400, -0.3557, -0.4040, -2.1382,  0.9083,\n",
      "         -1.5803,  0.4082,  0.2168, -0.7330, -0.6276,  0.0060,  0.4811, -0.5185,\n",
      "         -1.0411, -1.1945, -2.3071,  1.5369, -0.1449, -0.8100, -0.1580, -1.1141,\n",
      "         -0.7603, -1.8430, -0.6418, -0.3333,  0.2962, -0.4168,  1.3184,  0.9924]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ResNet.forward() got an unexpected keyword argument 'pseudo_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m      2\u001b[0m input_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m lora_resnet18(input_x)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/project/lt_problem/model/lora.py:256\u001b[0m, in \u001b[0;36mLora.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# 保存伪标签到类里或其它地方（比如 self.pseudo_index），不要再传给整个模型\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpseudo_index\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pseudo_label\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1600\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1606\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1607\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1608\u001b[0m     ):\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ResNet.forward() got an unexpected keyword argument 'pseudo_index'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "input_x = torch.randn(1, 3, 224, 224)\n",
    "output = lora_resnet18(input_x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First forward output: torch.Size([2, 1000])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ResNet.forward() got an unexpected keyword argument 'pseudo_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_x)  \u001b[38;5;66;03m# 第一次 forward：生成 pseudo_index\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst forward output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 118\u001b[0m output2 \u001b[38;5;241m=\u001b[39m model(input_x, pseudo_index\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1000\u001b[39m))  \u001b[38;5;66;03m# 第二次 forward：传入伪标签\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSecond forward output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output2\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 100\u001b[0m, in \u001b[0;36mLora.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_pseudo_index \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpseudo_index\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: ResNet.forward() got an unexpected keyword argument 'pseudo_index'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "\n",
    "# ------------------------------\n",
    "# 1. 定义 LoRA Linear 层\n",
    "# ------------------------------\n",
    "class LoraLinear(nn.Module):\n",
    "    def __init__(self, base_layer, num_class=1000, rank=8, alpha=16, top_k=1):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        self.num_class = num_class\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.top_k = top_k\n",
    "\n",
    "        # 冻结原始参数\n",
    "        for param in base_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        in_features = base_layer.in_features\n",
    "        out_features = base_layer.out_features\n",
    "\n",
    "        self.lora_A = nn.Parameter(torch.empty((num_class * rank, in_features)))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((out_features, num_class * rank)))\n",
    "        self.scaling = alpha / rank\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "\n",
    "        self.bias = base_layer.bias\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def _topk_lora(self, pseudo_index):\n",
    "        topk_value, topk_index = torch.topk(pseudo_index, self.top_k, dim=1)\n",
    "        top_index = topk_index.unsqueeze(2).expand(-1, -1, self.rank)\n",
    "        return top_index.reshape(-1)\n",
    "\n",
    "    def forward(self, x, pseudo_index=None):\n",
    "        if pseudo_index is None:\n",
    "            return self.base_layer(x)\n",
    "        else:\n",
    "            orig_output = self.base_layer(x)\n",
    "            top_index = self._topk_lora(pseudo_index)\n",
    "\n",
    "            A = self.lora_A[top_index]\n",
    "            A = A.view(-1, self.top_k * self.rank, A.size(-1))\n",
    "\n",
    "            B = self.lora_B[:, top_index]\n",
    "            B = B.view(B.size(0), -1, self.top_k * self.rank)\n",
    "            B = B.permute(1, 0, 2)\n",
    "\n",
    "            delta_W = torch.bmm(B, A)\n",
    "            lora_output = (x.unsqueeze(1) @ delta_W.transpose(1, 2)).squeeze(1) * self.scaling\n",
    "\n",
    "            return orig_output + lora_output + (self.bias if self.bias is not None else 0)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. LoRA Wrapper\n",
    "# ------------------------------\n",
    "class Lora(nn.Module):\n",
    "    def __init__(self, model, rank=8, alpha=16, top_k=1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.top_k = top_k\n",
    "        self._current_pseudo_index = None\n",
    "        self.hooks = []\n",
    "\n",
    "        self._apply_lora(self.model)\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _apply_lora(self, module):\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, nn.Linear):\n",
    "                lora_linear = LoraLinear(child, num_class=1000, rank=self.rank, alpha=self.alpha, top_k=self.top_k)\n",
    "                setattr(module, name, lora_linear)\n",
    "            else:\n",
    "                self._apply_lora(child)\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, LoraLinear):\n",
    "                def hook_fn(mod, inputs, kwargs):\n",
    "                    if 'pseudo_index' not in kwargs and self._current_pseudo_index is not None:\n",
    "                        kwargs['pseudo_index'] = self._current_pseudo_index\n",
    "                    return inputs, kwargs\n",
    "                handle = module.register_forward_pre_hook(hook_fn, with_kwargs=True)\n",
    "                self.hooks.append(handle)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        if 'pseudo_index' not in kwargs:\n",
    "            with torch.no_grad():\n",
    "                pseudo_index = self.model(x)\n",
    "            self._current_pseudo_index = pseudo_index\n",
    "            return self.model(x, *args, **kwargs)\n",
    "        else:\n",
    "            self._current_pseudo_index = kwargs['pseudo_index']\n",
    "            return self.model(x, *args, **kwargs)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. 示例：包装 ResNet18\n",
    "# ------------------------------\n",
    "def build_lora_resnet18(rank=8, alpha=16, top_k=1):\n",
    "    base_model = models.resnet18(pretrained=False)\n",
    "    return Lora(base_model, rank=rank, alpha=alpha, top_k=top_k)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. 测试运行\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_lora_resnet18()\n",
    "    input_x = torch.randn(2, 3, 224, 224)\n",
    "    output = model(input_x)  # 第一次 forward：生成 pseudo_index\n",
    "    print(\"First forward output:\", output.shape)\n",
    "\n",
    "    output2 = model(input_x, pseudo_index=torch.rand(2, 1000))  # 第二次 forward：传入伪标签\n",
    "    print(\"Second forward output:\", output2.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
